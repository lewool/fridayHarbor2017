{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural image decomposition and hierarchical clustering\n",
    "### Lauren E. Wool, Summer Workshop on the Dynamic Brain (September 2, 2017)\n",
    "\n",
    "#### This code imports the Brain Observatory stimulus set of natural images, and decomposes them based on a preset bank of Gabor filters. Once decomposed, the convolved images are clustered using HCA or K-means clustering into \"similar\" groups. These groupings inform a new index by which to order the image indices for the representational similarity matrices (RSMs). \n",
    "#### As an alternative, the convolved images are clustered based on 'relevant pixels'â€”that is, the areas of the images that are 'seen' by the population receptive field. This produces a new image index for each experiment, by which the RSMs can be shuffled. \n",
    "#### If the new indexing order is somehow more informative that the arbitrary starting order, one would expect to see more 'clumping' in the RSM (similar to how neuron correlation 'clumps' in the static-grating RSMs over similar SFs, orientations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Imports, drive path, functions</h2>\n",
    "\n",
    "<p>Set up your environment with the modules and functions you need\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/dynamic-brain-workshop/brain_observatory_cache/brain_observatory_manifest.json\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import cv2\n",
    "from skimage import filters\n",
    "from scipy.ndimage.filters import convolve\n",
    "from scipy.misc import imresize\n",
    "import swdb2017.brain_observatory.utilities.Plot_cell_rf_and_image as crfimage\n",
    "import swdb2017.brain_observatory.utilities.Plot_population_rf_and_image as prfimage\n",
    "import swdb2017.brain_observatory.get_brain_observatory_expdata as gboe\n",
    "import allensdk.brain_observatory.stimulus_info as si\n",
    "from allensdk.brain_observatory.observatory_plots import plot_mask_outline\n",
    "from scipy import signal\n",
    "from scipy import misc\n",
    "import timeit\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "# AWS\n",
    "drive_path = '/data/dynamic-brain-workshop/brain_observatory_cache/'\n",
    "# drive_path = '/Volumes/Brain2017/data/dynamic-brain-workshop/brain_observatory_cache/'\n",
    "\n",
    "# Import the Brain Observatory cache from allensdk.core.brain_observatory_cache\n",
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "\n",
    "manifest_file = os.path.join(drive_path,'brain_observatory_manifest.json')\n",
    "print manifest_file\n",
    "\n",
    "boc = BrainObservatoryCache(manifest_file=manifest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def fft_convolve(image,g_filter):\n",
    "    conv = signal.fftconvolve(image, g_filter, mode='same')\n",
    "    image_height = image.shape[0]\n",
    "    image_length = image.shape[1]\n",
    "    crop0 = int(image.shape[0]*.1)\n",
    "    crop1 = int(image.shape[1]*.1)\n",
    "    conv_cropped = conv[crop0:image_height-crop0,crop1:image_length-crop1]\n",
    "    amp = (np.amax(np.abs(conv_cropped)))\n",
    "    amp2 = np.amax(conv_cropped.real)-np.amin(conv_cropped.real)\n",
    "    return conv_cropped, amp, amp2\n",
    "\n",
    "def fft_convolve_no_crop(image,g_filter):\n",
    "    conv = signal.fftconvolve(image, g_filter, mode='same')\n",
    "    amp = (np.amax(np.abs(conv)))\n",
    "    amp2 = np.amax(conv.real)-np.amin(conv.real)\n",
    "    return conv, amp, amp2\n",
    "\n",
    "def fft_convolve_amp_only(image,g_filter):\n",
    "    conv = signal.fftconvolve(image, g_filter, mode='full')\n",
    "    amp = (np.amax(np.abs(conv)))\n",
    "    amp2 = np.amax(conv.real)-np.amin(conv.real)\n",
    "    return amp, amp2\n",
    "\n",
    "def downsample_image(image, percent):\n",
    "    image_height = int(image.shape[0])\n",
    "    image_length = int(image.shape[1])\n",
    "    p = percent/100.\n",
    "    im_ds = imresize(image, percent, interp='nearest', mode=None)\n",
    "    return im_ds\n",
    "    \n",
    "def exp_id_tool(number):\n",
    "    if number > 199:\n",
    "        return exp_ids.index(number)\n",
    "    else: \n",
    "        return exp_ids[number]\n",
    "\n",
    "def scene_and_experiment_pop_rf(boc, experiment_id):\n",
    "    pop_rf = get_population_rf(boc, experiment_id)\n",
    "    m = si.BrainObservatoryMonitor()\n",
    "    pop_rf[pop_rf>0]=1\n",
    "    rf_convert = m.lsn_image_to_screen(pop_rf, origin='upper')\n",
    "    rf_convert[np.where(rf_convert<200)]=0\n",
    "    return rf_convert\n",
    "    \n",
    "def get_natural_scene_template_expt(boc, experiment_id):\n",
    "    ns_session_id = boc.get_ophys_experiments(experiment_container_ids=[experiment_id], \n",
    "                                              stimuli=['natural_scenes'])[0]['id']\n",
    "    data_set_ns = boc.get_ophys_experiment_data(ophys_experiment_id=ns_session_id)\n",
    "    ns_template = data_set_ns.get_stimulus_template('natural_scenes')\n",
    "    return ns_template\n",
    "\n",
    "def get_population_rf(boc, experiment_id):\n",
    "    c_flag = 'C'\n",
    "    lsn_name = 'locally_sparse_noise'\n",
    "    rf_name = 'receptive_field_lsn'\n",
    "\n",
    "    for a in boc.get_ophys_experiments(experiment_container_ids=[experiment_id]):\n",
    "        if a['session_type'].endswith('2'):\n",
    "            c_flag = 'C2'\n",
    "            if a['targeted_structure'] != 'VISp':\n",
    "                lsn_name = 'locally_sparse_noise_8deg'\n",
    "                rf_name = 'receptive_field_lsn8'\n",
    "            else:\n",
    "                lsn_name = 'locally_sparse_noise_4deg'\n",
    "                rf_name = 'receptive_field_lsn4'\n",
    "\n",
    "    drive_path = boc.manifest.get_path('BASEDIR')\n",
    "    if c_flag=='C':\n",
    "        session_id = boc.get_ophys_experiments(experiment_container_ids=[experiment_id], stimuli=[lsn_name])[0]['id']\n",
    "        analysis_file = os.path.join(drive_path, 'ophys_experiment_analysis', str(session_id)+'_three_session_C_analysis.h5')\n",
    "    elif c_flag=='C2':    \n",
    "        session_id = boc.get_ophys_experiments(experiment_container_ids=[experiment_id], stimuli=[lsn_name])[0]['id']\n",
    "        analysis_file = os.path.join(drive_path, 'ophys_experiment_analysis', str(session_id)+'_three_session_C2_analysis.h5')\n",
    "    \n",
    "    f = h5py.File(analysis_file, 'r')\n",
    "    receptive_field = f['analysis'][rf_name].value\n",
    "    f.close()\n",
    "    pop_rf = np.nansum(receptive_field, axis=(2,3))\n",
    "    return pop_rf\n",
    "\n",
    "def get_pop_rf_amps(boc, experiment_id, conv):\n",
    "    conv_test = conv.real\n",
    "    rf_convert = scene_and_experiment_pop_rf(boc, experiment_id)\n",
    "    rf_resize = misc.imresize(rf_convert, 25)\n",
    "    rf_resize = rf_resize/255\n",
    "    pad_x = int(np.floor((rf_resize.shape[1]-conv_test.shape[1])/2))\n",
    "    pad_y = int(np.floor((rf_resize.shape[0]-conv_test.shape[0])/2))\n",
    "    npad = ((pad_y, pad_y+1), (pad_x, pad_x+1))\n",
    "    conv_pad = np.pad(conv_test, pad_width=npad, mode='constant', constant_values=0)\n",
    "    conv_filtered = conv_pad*rf_resize\n",
    "    amp = np.amax(conv_filtered)-np.amin(conv_filtered)\n",
    "    return conv_filtered, amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the representational similarity matrix information and save it\n",
    "rs = gboe.get_all_representational_similarity_matrices(boc, 'three_session_B', 'natural_scenes', drive_path)\n",
    "\n",
    "import json \n",
    "from swdb2017.general_tools.json_encoder import MyEncoder\n",
    "filename ='/home/laurenw/repsimmat.json'\n",
    "with open(filename, 'w') as data_file:\n",
    "    data_file.write(json.dumps(rs,filename,indent = 4, ensure_ascii = True, cls = MyEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the representational similarity matrix data (if you've saved it as per above)\n",
    "\n",
    "import json \n",
    "from swdb2017.general_tools.json_encoder import MyEncoder\n",
    "filename ='/home/laurenw/repsimmat.json'\n",
    "with open(filename,'r') as data_file:\n",
    "    rs = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a 3D array of all the representational similarity matrices \n",
    "\n",
    "rs_all = np.zeros((199,118,118))\n",
    "for i in range(len(rs['representational_similarity'])):\n",
    "    r = np.array(rs['representational_similarity'][i])    \n",
    "    r = r[1:119,1:119] #remove the blank image frame from the experiment's RSM\n",
    "    np.fill_diagonal(r,np.nan)\n",
    "    r[~np.isnan(r)] = stats.mstats.zscore(r[~np.isnan(r)])\n",
    "    rs_all[i,:,:] = r  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieve information for each natural-scenes experiment\n",
    "session_type = 'three_session_B'\n",
    "exps = boc.get_experiment_containers(file_name=None, ids=None, \n",
    "                                         targeted_structures=None, \n",
    "                                         imaging_depths=None, \n",
    "                                         cre_lines=None, \n",
    "                                         transgenic_lines=None, \n",
    "                                         include_failed=False)\n",
    "first_session_id = boc.get_ophys_experiments(\n",
    "                experiment_container_ids=[exps[0]['id']], \n",
    "                session_types = [session_type])[0]['id']\n",
    "\n",
    "# Generate a list of experiment IDs (experiment container ID) and session IDs (natural-scenes experiment ID)\n",
    "exp_ids = []\n",
    "session_ids = []\n",
    "for e in range(len(exps)):\n",
    "    exp_ids.append(exps[e]['id'])\n",
    "    session_id = boc.get_ophys_experiments(\n",
    "                experiment_container_ids=[exps[e]['id']], \n",
    "                session_types = [session_type])[0]['id']\n",
    "    session_ids.append(session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Initial image processing</h2>\n",
    "\n",
    "<p>Load the images and create filters\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the experimental data for a single experiment (any will do)\n",
    "data_set = boc.get_ophys_experiment_data(501498760)\n",
    "\n",
    "# Read in a 3D array of images (shape: n_images,image_height,image_length)\n",
    "scenes = data_set.get_stimulus_template('natural_scenes')\n",
    "\n",
    "# Downsample the images\n",
    "p = 0.25\n",
    "scenes_ds = np.zeros((scenes.shape[0],int(scenes.shape[1]*p),int(scenes.shape[2]*p)))\n",
    "for sidx,s in enumerate(scenes):\n",
    "    im_ds = downsample_image(s,25)\n",
    "    scenes_ds[sidx] = im_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create gabor filters\n",
    "'''for 0.1 cycles/deg, thatâ€™s 10 degrees needed for a full cycle\n",
    "293 px/96 deg =  3.1 px/deg (96 degrees = screen width)\n",
    "96 deg/293px = 0.33 deg/px\n",
    "if you need 10 degrees to display a full cycle, then you need 31 pixels\n",
    "0.1 cycles/deg = 0.1 cycles/3.1 px = 0.03 cycles/px'''\n",
    "\n",
    "sf = np.array([0.02, 0.04, 0.08, 0.16, 0.32]) # cycles/deg\n",
    "ori = np.array([0., 30., 60., 90., 120., 150.]) # deg\n",
    "\n",
    "# Correct the values based on the inputs required for skimage.filters\n",
    "sf_rad = (sf/180)*np.pi\n",
    "ori_rad = (ori/180)*np.pi\n",
    "sf_px = sf/3.1 \n",
    "\n",
    "# Global filter parameters\n",
    "n_stds = 3\n",
    "bw = 10\n",
    "\n",
    "# Initialize the list of gabors\n",
    "gabor_list = []\n",
    "\n",
    "# Loop through the sf and ori parameter arrays and build the filters\n",
    "for sidx,s in enumerate(sf_px):\n",
    "    sigma_x = 1/sf_px[sidx] * .5\n",
    "    sigma_y = 2*sigma_x\n",
    "    for oidx,o in enumerate(ori_rad):\n",
    "        # Build a gabor filter\n",
    "        gabor = filters.gabor_kernel(s, o, bw, sigma_x, sigma_y, n_stds, 0)\n",
    "        # Append it to \"gabor_list\"\n",
    "        gabor_list.append(gabor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show a gabor-filtered image and the original image\n",
    "\n",
    "image = scenes_ds[50]\n",
    "gfilter = gabor_list[25]\n",
    "output, amp, amp2 = fft_convolve_no_crop(image,gfilter)\n",
    "print(amp2)\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121),plt.imshow(image, cmap = 'gray')\n",
    "ax2 = fig.add_subplot(122),plt.imshow(output.real, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Image analysis: images only</h2>\n",
    "\n",
    "<p>Convolutions and clustering analysis\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Iteratively convolve all the filters with all the images\n",
    "\n",
    "n_images = scenes_ds.shape[0]\n",
    "values_array = np.zeros((n_images,len(gabor_list)))\n",
    "all_convs = []\n",
    "for sidx,s in enumerate(scenes_ds):\n",
    "    for gidx,g in enumerate(gabor_list):\n",
    "        a = s\n",
    "        b = g\n",
    "        conv, amp, amp2 = fft_convolve(a,b)\n",
    "        values_array[sidx,gidx] = amp2\n",
    "        all_convs.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do hierarchical clustering analysis of the image x filter array\n",
    "# Then plot the dendrogram\n",
    "Z = linkage(values_array, 'ward')\n",
    "plt.figure()\n",
    "plt.title('Clustering scenes by Gabor filters')\n",
    "plt.xlabel('natural scene index')\n",
    "plt.ylabel('cluster distance')\n",
    "R = dendrogram(Z,leaf_rotation=90., leaf_font_size=6.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try k-means clustering too, to see what you get (you have to manually define how many clusters you want)\n",
    "\n",
    "k = KMeans(n_clusters=4, random_state=0).fit_predict(values_array)\n",
    "k_sorted = np.argsort(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Image analysisâ€”receptive fields</h2>\n",
    "\n",
    "<p>Mask the convolution arrays by the population receptive field for a given experiment, then assess the magnitude from that.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Iteratively convolve all the filters with all the images\n",
    "\n",
    "n_images = scenes_ds.shape[0]\n",
    "values_array = np.zeros((n_images,len(gabor_list)))\n",
    "all_convs = []\n",
    "for sidx,s in enumerate(scenes_ds):\n",
    "    for gidx,g in enumerate(gabor_list):\n",
    "        a = s\n",
    "        b = g\n",
    "        conv, amp, amp2 = fft_convolve_no_crop(a,b)\n",
    "        values_array[sidx,gidx] = amp2\n",
    "        all_convs.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the convolution magnitudes for all the experiments (contstrained by population RF in each)\n",
    "# (If you haven't already done this, generate the arrays with the cell below)\n",
    "rf_values_array = np.load('rf_values_array.npy')\n",
    "rf_values_array_sums = np.load('rf_values_array_sums.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_exps = len(exp_ids)\n",
    "n_images = scenes_ds.shape[0]\n",
    "n_filters = len(gabor_list)\n",
    "rf_values_array = np.zeros((n_exps, n_images,n_filters))\n",
    "rf_values_array_sums = np.zeros((n_exps, n_images,n_filters))\n",
    "\n",
    "for eidx,e in enumerate(exp_ids):\n",
    "    try:\n",
    "        rf_convert = scene_and_experiment_pop_rf(boc, e)\n",
    "        rf_resize = misc.imresize(rf_convert, 25)\n",
    "        rf_resize = rf_resize/255\n",
    "    except:\n",
    "        rf_resize = np.zeros((300,480))\n",
    "    print(eidx)\n",
    "    for s in range(n_images):\n",
    "        for g in range(n_filters):\n",
    "            a = all_convs[s*len(gabor_list)+g].real\n",
    "            pad_x = int(np.floor((rf_resize.shape[1]-a.shape[1])/2))\n",
    "            pad_y = int(np.floor((rf_resize.shape[0]-a.shape[0])/2))\n",
    "            npad = ((pad_y, pad_y+1), (pad_x, pad_x+1))\n",
    "            conv_pad = np.pad(a, pad_width=npad, mode='constant', constant_values=0)\n",
    "            conv_filtered = conv_pad*rf_resize\n",
    "            amp = np.amax(conv_filtered)-np.amin(conv_filtered)\n",
    "            ampsum = np.sum(abs(conv_filtered))\n",
    "            rf_values_array[eidx,s,g] = amp\n",
    "            rf_values_array_sums[eidx,s,g] = ampsum\n",
    "\n",
    "np.save('rf_values_array.npy',rf_values_array)\n",
    "np.save('rf_values_array_sums.npy',rf_values_array_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do hierarchical clustering analysis of the image x filter slice of the 199-experiment array\n",
    "# Each element of Z is the linkage info for each experiment\n",
    "# Each element of R is the RSM shuffling index for each experiment\n",
    "Z_rf = []\n",
    "R_rf = []\n",
    "for v in rf_values_array_sums:\n",
    "    Z = linkage(v, 'ward')\n",
    "    Z_rf.append(Z)\n",
    "    R = dendrogram(Z, p=30, truncate_mode=None, color_threshold=None, \n",
    "                   get_leaves=True, orientation='top', labels=None, count_sort=False, \n",
    "                   distance_sort=False, show_leaf_counts=False, no_plot=True)\n",
    "    R_rf.append(R['leaves'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the dendrogram for a particular experiment\n",
    "\n",
    "Z = linkage(rf_values_array[63], 'ward')\n",
    "plt.figure()\n",
    "plt.title('Clustering scenes by Gabor filters')\n",
    "plt.xlabel('natural scene index')\n",
    "plt.ylabel('cluster distance')\n",
    "R = dendrogram(Z,leaf_rotation=90., leaf_font_size=6.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try k-means clustering too, to see what you get (you have to manually define how many clusters you want)\n",
    "# Each element in K_sorted is the RSM shuffling index for each experiment\n",
    "K_sorted = []\n",
    "for v in rf_values_array_sums:\n",
    "    k = KMeans(n_clusters=4, random_state=0).fit_predict(v)\n",
    "    K_sorted.append(np.argsort(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Representational similarity matrix analysis: images only</h2>\n",
    "\n",
    "<p>Compare the experimental matrix to a shuffled matrix based on the clustering done above\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reshuffle the RSM based on a clustering output index\n",
    "def shuffle_RSM(rs_all,idx):\n",
    "    rs2 = np.copy(rs_all)\n",
    "    idx1 = idx\n",
    "    rs_Z = np.zeros((199,118,118))\n",
    "    for ridx,r in enumerate(rs2):\n",
    "        test_rs = np.copy(r)\n",
    "        test_rs = test_rs[idx1,:]\n",
    "        test_rs = test_rs[:,idx1]\n",
    "        rs_Z[ridx,:,:] = test_rs\n",
    "    return rs_Z\n",
    "\n",
    "rs_Z = shuffle_RSM(rs_all, R['leaves'])\n",
    "rs_K = shuffle_RSM(rs_all, k_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the comparison of the clustering output versus the original RSM\n",
    "\n",
    "def plot_RSM_comparison(original_rsm, rs_Z, rs_K, rs_index):\n",
    "    plt.figure()\n",
    "    plt.subplot(131),plt.imshow(original_rsm[rs_index],cmap='viridis',vmin=-2, vmax=2)\n",
    "    plt.subplot(132),plt.imshow(rs_Z[rs_index],cmap='viridis',vmin=-2, vmax=2)\n",
    "    plt.subplot(133),plt.imshow(rs_K[rs_index],cmap='RdBu',vmin=-2, vmax=2)\n",
    "\n",
    "plot_RSM_comparison(rs_original, rs_Z, rs_K, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Representational similarity matrix analysis: RF-specific</h2>\n",
    "\n",
    "<p>Compare the experimental matrix to a shuffled matrix based on the clustering done above\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reshuffle the RSM based on a clustering output index\n",
    "def shuffle_RSM_unique_rfs(rs_all,idx):\n",
    "    rs_Z = np.zeros((199,118,118))\n",
    "    for ridx,r in enumerate(rs_all):\n",
    "        idx1 = idx[ridx]\n",
    "        test_rs = r\n",
    "        test_rs = test_rs[idx1,:]\n",
    "        test_rs = test_rs[:,idx1]\n",
    "        rs_Z[ridx,:,:] = test_rs\n",
    "    return rs_Z\n",
    "\n",
    "rs_Z = shuffle_RSM_unique_rfs(rs_all, R_rf)\n",
    "rs_K = shuffle_RSM_unique_rfs(rs_all, K_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_RSM_comparison(original_rsm, rs_Z, rs_K, rs_index):\n",
    "    plt.figure()\n",
    "    plt.subplot(131),plt.imshow(original_rsm[rs_index], vmin=-2, vmax=2)\n",
    "    plt.subplot(132),plt.imshow(rs_Z[rs_index],vmin=-2, vmax=2)\n",
    "    plt.subplot(133),plt.imshow(rs_K[rs_index], vmin=-2, vmax=1.5)\n",
    "    plt.savefig('RF' + str(rs_index) + '.pdf')\n",
    "\n",
    "for i in range(199):    \n",
    "    plot_RSM_comparison(rs_all, rs_Z, rs_K, i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
